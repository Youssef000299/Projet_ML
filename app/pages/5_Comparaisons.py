import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go

from dataset.state import DatasetState
from dataset.forms.config import dataset_config_form
import utils.training as train  # Suppose que train_and_evaluate_model(model) retourne (model, y_pred, cv_scores, report, conf_matrix)
from utils.model_storage import ModelStorage

# ---------------------------------------------------------------------
# Style CSS personnalis√©
# ---------------------------------------------------------------------
st.markdown("""
    <style>
        .section-header {
            background: linear-gradient(135deg, #6a11cb, #2575fc);
            padding: 1.5rem;
            border-radius: 8px;
            color: white;
            text-align: center;
            margin-bottom: 2rem;
            font-family: 'Arial', sans-serif;
        }
        .info-box {
            background-color: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            border-radius: 10px;
            padding: 1rem;
            margin: 1rem 0;
            color: #FFFFFF;
            font-family: 'Arial', sans-serif;
        }
        .info-box h4 {
            margin-bottom: 0.5rem;
        }
        .metrics-container {
            background-color: rgba(255, 255, 255, 0.05);
            border-radius: 10px;
            padding: 1rem;
            margin: 1rem 0;
        }
    </style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------------------
# En-t√™te de la page
# ---------------------------------------------------------------------
st.markdown('<div class="section-header"><h1>üîç Comparaison des Mod√®les</h1></div>', unsafe_allow_html=True)
st.markdown("""
Cette page compare les diff√©rents mod√®les selon plusieurs m√©triques.  
Chaque onglet propose un graphique comparatif pour la m√©trique choisie et un commentaire 
pour indiquer le meilleur mod√®le.
""")

# ---------------------------------------------------------------------
# V√©rification du dataset
# ---------------------------------------------------------------------
dataset = DatasetState.get_dataset()
if dataset is None:
    st.markdown("### :ghost: Aucun jeu de donn√©es")
    st.caption("Chargez un jeu de donn√©es pour pouvoir effectuer la comparaison.")
    st.button("üì§ Charger un Dataset", type="primary", on_click=dataset_config_form, use_container_width=True)
    st.stop()

# V√©rification si des mod√®les ont √©t√© entra√Æn√©s
if not ModelStorage.has_trained_models():
    st.warning("‚ö†Ô∏è Aucun mod√®le n'a √©t√© entra√Æn√©. Veuillez d'abord entra√Æner des mod√®les dans la page Entra√Ænement.")
    st.info("üëà Allez √† la page 'Entra√Ænement' pour entra√Æner des mod√®les.")
    st.stop()

# R√©cup√©ration des r√©sultats des mod√®les entra√Æn√©s
results_dict = ModelStorage.get_model_results()

# Cr√©ation des DataFrames pour la comparaison
accuracy_data = []
precision_data = []
recall_data = []
f1_data = []
cv_data = []

for model_name, results in results_dict.items():
    accuracy_data.append({"Mod√®le": model_name, "Valeur": results["accuracy"]})
    precision_data.append({"Mod√®le": model_name, "Valeur": results["precision"]})
    recall_data.append({"Mod√®le": model_name, "Valeur": results["recall"]})
    f1_data.append({"Mod√®le": model_name, "Valeur": results["f1"]})
    cv_data.append({"Mod√®le": model_name, "Valeur": results["cv_scores"].mean()})

df_accuracy = pd.DataFrame(accuracy_data)
df_precision = pd.DataFrame(precision_data)
df_recall = pd.DataFrame(recall_data)
df_f1 = pd.DataFrame(f1_data)
df_cv = pd.DataFrame(cv_data)

# Fonction pour cr√©er un graphique de comparaison
def create_comparison_chart(df, title, y_label):
    fig = px.bar(
        df, 
        x="Mod√®le", 
        y="Valeur", 
        color="Mod√®le",
        title=title,
        labels={"Valeur": y_label}
    )
    fig.update_layout(
        template="plotly_dark",
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0.05)'
    )
    return fig

# Cr√©ation des onglets pour chaque m√©trique
tab_acc, tab_prec, tab_rec, tab_f1, tab_cv = st.tabs([
    "Accuracy", 
    "Precision", 
    "Recall", 
    "F1 Score", 
    "Cross-Validation"
])

# Onglet Accuracy
with tab_acc:
    st.subheader("Comparaison par Accuracy")
    fig_acc = create_comparison_chart(df_accuracy, "Accuracy des mod√®les", "Accuracy")
    st.plotly_chart(fig_acc, use_container_width=True)
    
    # Meilleur mod√®le
    best_acc = df_accuracy["Valeur"].max()
    best_model_acc = df_accuracy.loc[df_accuracy["Valeur"] == best_acc, "Mod√®le"].iloc[0]
    st.markdown(f"**Le meilleur mod√®le selon l'Accuracy est {best_model_acc} avec un score de {best_acc:.3f}.**")

# Onglet Precision
with tab_prec:
    st.subheader("Comparaison par Precision")
    fig_prec = create_comparison_chart(df_precision, "Precision des mod√®les", "Precision")
    st.plotly_chart(fig_prec, use_container_width=True)
    
    # Meilleur mod√®le
    best_prec = df_precision["Valeur"].max()
    best_model_prec = df_precision.loc[df_precision["Valeur"] == best_prec, "Mod√®le"].iloc[0]
    st.markdown(f"**Le meilleur mod√®le selon la Precision est {best_model_prec} avec un score de {best_prec:.3f}.**")

# Onglet Recall
with tab_rec:
    st.subheader("Comparaison par Recall")
    fig_rec = create_comparison_chart(df_recall, "Recall des mod√®les", "Recall")
    st.plotly_chart(fig_rec, use_container_width=True)
    
    # Meilleur mod√®le
    best_rec = df_recall["Valeur"].max()
    best_model_rec = df_recall.loc[df_recall["Valeur"] == best_rec, "Mod√®le"].iloc[0]
    st.markdown(f"**Le meilleur mod√®le selon le Recall est {best_model_rec} avec un score de {best_rec:.3f}.**")

# Onglet F1 Score
with tab_f1:
    st.subheader("Comparaison par F1 Score")
    fig_f1 = create_comparison_chart(df_f1, "F1 Score des mod√®les", "F1 Score")
    st.plotly_chart(fig_f1, use_container_width=True)
    
    # Meilleur mod√®le
    best_f1 = df_f1["Valeur"].max()
    best_model_f1 = df_f1.loc[df_f1["Valeur"] == best_f1, "Mod√®le"].iloc[0]
    st.markdown(f"**Le meilleur mod√®le selon le F1 Score est {best_model_f1} avec un score de {best_f1:.3f}.**")

# Onglet Cross-Validation
with tab_cv:
    st.subheader("Comparaison par Cross-Validation")
    fig_cv = create_comparison_chart(df_cv, "Cross-Validation des mod√®les", "CV Score")
    st.plotly_chart(fig_cv, use_container_width=True)
    
    # Meilleur mod√®le
    best_cv = df_cv["Valeur"].max()
    best_model_cv = df_cv.loc[df_cv["Valeur"] == best_cv, "Mod√®le"].iloc[0]
    st.markdown(f"**Le meilleur mod√®le selon la Cross-Validation est {best_model_cv} avec un score de {best_cv:.3f}.**")

# Tableau r√©capitulatif
st.subheader("Tableau r√©capitulatif des performances")
summary_data = []
for model_name, results in results_dict.items():
    summary_data.append({
        "Mod√®le": model_name,
        "Accuracy": f"{results['accuracy']:.3f}",
        "Precision": f"{results['precision']:.3f}",
        "Recall": f"{results['recall']:.3f}",
        "F1 Score": f"{results['f1']:.3f}",
        "CV Score": f"{results['cv_scores'].mean():.3f}"
    })

summary_df = pd.DataFrame(summary_data)
st.dataframe(summary_df, use_container_width=True)

# Recommandation finale
st.markdown("""
    <div class="info-box">
        <h4>üìä Recommandation finale</h4>
        <p>En fonction des diff√©rentes m√©triques, voici notre recommandation :</p>
    </div>
""", unsafe_allow_html=True)

# Calcul du score global (moyenne des rangs pour chaque m√©trique)
model_ranks = {}
for model in results_dict.keys():
    model_ranks[model] = 0

# Calcul des rangs pour chaque m√©trique
for df, weight in [(df_accuracy, 1), (df_precision, 1), (df_recall, 1), (df_f1, 1.5), (df_cv, 1.5)]:
    df_sorted = df.sort_values("Valeur", ascending=False)
    for i, model in enumerate(df_sorted["Mod√®le"]):
        model_ranks[model] += (i + 1) * weight

# Mod√®le avec le meilleur rang global
best_model = min(model_ranks.items(), key=lambda x: x[1])[0]

st.success(f"**Le mod√®le recommand√© est : {best_model}**")
st.markdown(f"""
    Ce mod√®le offre le meilleur √©quilibre entre les diff√©rentes m√©triques d'√©valuation.
    Pour une utilisation en production, nous recommandons d'utiliser ce mod√®le.
""")
